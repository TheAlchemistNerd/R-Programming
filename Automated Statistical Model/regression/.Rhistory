table(mtcars$cyl,mtcars$am)
with(mtcars,
table(cyl, am))
cyl_am <- with(mtcars,
table(cyl, am))
cyl_am
cyl_am %>%
Total <- rbind(sum(Automatic), sum(Manual))
cyl_am %>%
Total <- rbind(sum(mtcars$Automatic), sum(mtcars$Manual))
with(cyl_am,
rbind(sum(mtcars$Automatic), sum(mtcars$Manual)))
rbind(c(sum(mtcars$Automatic), sum(mtcars$Manual)))
with(cyl_am,
rbind(c(sum(mtcars$Automatic), sum(mtcars$Manual))))
data <- c(45, 48, 62, 70, 73, 74, 74, 78, 78, 79, 79, 80, 80, 80, 80, 80, 80, 81, 82, 82)
data
plot(data)
plot(data, ylim = c(40, 85))
boxplot(data)
boxplot(data, ylim = c(40, 85))
fivenum(data)
dnorm(1.25, lower.tail = T)
library(tigerstats)
pnormGC(c(25, 30), region = "between", mean = 22, sd = 4, graph = T)
library(tigerstats)
pnormGC(c(25, 30), region = "between", mean = 22, sd = 4, graph = T)
pnormGC(30, region = "above", mean = 22, sd = 4, graph = T)
qnormGC(0.05, region = "below",mean = 22, sd = 4, graph = T)
pnormGC(50, region = "above", mean = 49, sd = 5, graph = T)
pnormGC(8, region = "below", mean = 7.65, sd = 0.1954, graph = T)
pnormGC(8, region = "below", mean = 7.65, sd = 2.15, graph = T)
pnormGC(50, region = "above", mean = 49, sd = 0.559, graph = T)
qnorm(0.10)
qnorm(0.10,lower.tail = F)
qnormGC(0.1, region = "between",mean = 0, sd = 1, graph = T)
qnormGC(0.1, region = "between",mean = 6.78, sd = 1.77/sqrt(40), graph = T)
qnormGC(0.1, region = "between",mean = 7.62, sd = 1.77/sqrt(40), graph = T)
qnormGC(0.05, region = "between",mean = 7.62, sd = 1.77/sqrt(40), graph = T)
qnormGC(0.05, region = "between",mean = 6.78, sd = 1.77/sqrt(40), graph = T)
par(mfrow = c(3, 4))
# top panel
qqnorm(rnorm(10)); qqnorm(rnorm(10)); qqnorm(rnorm(10)); qqnorm(rnorm(10))
# middle panel
qqnorm(rnorm(100)); qqnorm(rnorm(100)); qqnorm(rnorm(100)); qqnorm(rnorm(100))
# bottom panel
qqnorm(rnorm(1000)); qqnorm(rnorm(1000)); qqnorm(rnorm(1000)); qqnorm(rnorm(1000))
data <- read.table("C:\\Users\\qwert\\Documents\\data.txt")
data
Mean = mean(data)
Mean
Median = median(data)
Median
STDEV = sd(data)
STDEV
str(data)
data_vec <- as.vector(data)
data_vec
rownames(data)
mean(data_vec)
mean(data_vec$V1)
mean(data$V1)
colnames(data)<- "entries"
data
with(data, mean(entries))
data <- read.table("C:\\Users\\qwert\\Documents\\data.txt")
data
# rename the column
colnames(data)<- "entries"
data
mean_data = with(data, mean(entries))
mean_data
median_data = with(data, median(entries))
median_data
sd_data = with(data, sd(entries))
sd_data
# frequency plot
with(data plot(entries)
# frequency plot
with(data,plot(entries))
with(data, table(entries))
View(with(data, table(entries)))
# frequency plot
freq_table <- with(data, table(entries))
View(freq_table)
plot(freq_table)
?plot()
class(freq_table)
with(freq_table, plot(x = entries, y = freq))
str(freq_table)
barplot(freq_table)
hist(freq_table)
View(freq_table)
with(freq_table, plot(x = entries, y = Freq))
plot(freq_table$entries, freq_table$Freq)
data_freq <- as.dataframe(freq_table)
data_freq <- asdata.frame(freq_table)
data_freq <- as.data.frame(freq_table)
plot(data_freq$entries, data_freq$Freq)
View(data_freq)
?hist()
hist(data_freq$entries, freq=True)
hist(data$entries, freq=True)
hist(data$entries, freq=TRUE)
freq_plot(hist(data$entries, freq=TRUE, prob = TRUE))
freq_plot<-hist(data$entries, freq=TRUE, prob = TRUE)
freq_plot<-hist(data$entries, freq=TRUE, prob = TRUE)
freq_plot<-hist(data$entries, freq=TRUE)
lines(density(freq_plot))
lines(density(data_freq$Freq))
q()
any(grepl("ggplot2", installed.packages()))
?grepl()
library("MASS")
library("ggplot2")
str(Cars93)
head(Cars93)
#Exploring the data.
ggplot(Cars93, aes(x=Horsepower))+
geom_histogram(color="black", fill="white",binwidth = 10)+
facet_wrap(~Origin)
#Exploring the data.
ggplot(Cars93, aes(x=Horsepower))+
geom_histogram(color="black", fill="white",binwidth = 10)+
facet_wrap(~Origin)
#Exploring the data.
ggplot(Cars93, aes(x=Horsepower))+
geom_histogram(color="black", fill="red",binwidth = 10)+
facet_wrap(~Origin)
ggplot(Cars93, aes(x=Horsepower)) +
geom_density() +
facet_wrap(~Origin)
library(Rcpp)
cppFunction(
'int add(int x, int y, int z){
int sum = x + y + z;
return sum;
}'
)
add(1, 2, 3)
cppFunction(
'int signC(int x) {
if (x > 0) {
return 1;
} else if (x == 0) {
return 0;
} else {
return -1;
}
)
cppFunction(
'int signC(int x) {
if (x > 0) {
return 1;
} else if (x == 0) {
return 0;
} else {
return -1;
}
}'
)
cppFunction(
'int signC(int x) {
if (x > 0) {
return 1;
} else if (x == 0) {
return 0;
} else {
return -1;
}
}'
)
signC(-6)
sumR <- function(x) {
total <- 0
for (i in seq_along(x)) {
total <- total + x[i]
}
total
}
cppFunction(
'double sumC(NumericVector x) {
int n = x.size();
double total = 0;
for (int i = 0; i < n; ++i) {
total += x[i];
}
return total;
}'
)
x <- runif(1e3)
bench::mark()
sum(x)
sumC(x)
sumR(x)
library(MASS)
library(ISLR)
fix(Boston)
names(Boston)
lm.fit=lm(medv~lstat, data=Boston)
lm.fit
summary(lm.fit)
# finding pieces of information stoed in lm.fit
names(lm.fit)
# extractor functions
coef(lm.fit)
cofint(lm.fit)
confint(lm.fit)
# producing CIs and PIs for the
# prediction of medv for a given value of lstat
predict(lm.fit, data.frame(lstat=(c(5, 10, 15))),
interval = "confidence")
predict(lm.fit, data.frame(lstat=(c(5, 10, 15))),
interval = "prediction")
plot(lstat, medv)
attach(Boston)
plot(lstat, medv)
plot(lstat, medv)
abline(lm.fit)
# Additional plotting objects
abline (lm.fit ,lwd =3)
abline (lm.fit ,lwd =3, col ="red ")
plot(lstat ,medv ,col ="red ")
plot(lstat ,medv ,pch =20)
plot(lstat ,medv ,pch ="+")
plot (1:20 ,1:20, pch =1:20)
# Diagnostic plots
par(mfrow =c(2,2))
plot(lm.fit)
plot(predict (lm.fit), residuals (lm.fit))
plot(predict (lm.fit), rstudent (lm.fit))
# Multiple Regression
lm.fit = lm(medv~lstat + age, data=Boston)
summary(lm.fit)
# short hand on all the variables contained in the Boston dataset
lm.fit=lm(medv~., data = Boston)
summary(lm.fit)
any(grepl("car", installed.packages()))
library("Car")
library("car")
summary(mm.fit1)
summary(lm.fit1)
lm.fit1 = lm(medv~. age, data=Boston)
summary(lm.fit1)
lm.fit1 = lm(medv~.-age, data=Boston)
summary(lm.fit1)
# Alternatively you can use the update() function
lm.fit1 = update(lm.fit, ~.-age)
summary(lm.fit1)
# Interaction terms
summary(lm(medv~lstat*age, data = Boston))
# Non-linear Transformation of the Predictors
lm.fit2 = lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
lm.fit = lm(medv~lstat)
anova(lm.fit, lm.fit2)
par(mfrow=c(2, 2))
plot(lm.fit2)
any(grepl("tensorflow", installed.packages()))
any(grepl("keras", installed.packages()))
any(grepl("reticulate", installed.packages()))
any(grepl("shinydashboard", installed.packages()))
library(caret)
library(tidyverse)
install.packages("tensorflow")
install.packages("tensorlow")
install.packages("tensorflow")
df2 <- read.csv("C:/Users/qwert/Documents/microchip.csv", header=F)
head(df2)
# Save 20 rows for testing
reserved = df2 %>% sample_n(20)
df2_train = df2 %>% setdiff(reserved)
dim(reserved)
dim(df2_train)
# Save the test data. We will upload this  to our siny app
# to get predictions
write.csv(reserved, "C:/Users/qwert/Documents/test_data.csv", row.names = F)
# Name columns
names(df2_train) = c("Test1", "Test2", "Label")
# Visualize the data
cols <- c("0" = "red", "1" = "blue")
df2_train %>% ggplot(aes(x = Test1, y = Test2, color = factor(Label))) +
geom_point(size = 4, shape = 19, alpha = 0.6) +
scale_colour_manual(values = cols,labels = c("Failed", "Passed"), name = "Test Result")
# Feature Engineering
feature_mapping = function(df){
new_data = c()
for( i in 1:6){
for(j in 0:i){
temp = (df$Test1)^i+(df$Test2)^(i-j)
new_data = cbind(new_data, temp)
}
}
colnames(new_data) = paste0("V", 1:ncol(new_data))
new_data
}
mapped = feature_mapping(df2_train)
head(mapped)
df_final = cbind(df2_train, mapped)
dim(df_final)
df_final$Label = as.factor(df_final$Label)
levels(df_final$Label) <- c("Failed", "Passed")
# Building a reguralized logistic regression model using cross-validation and grid search
set.seed(0)
cctrl1 <- trainControl(method = "cv", number = 10,
allowParallel = T,
summaryFunction = twoClassSummary,
classProbs = T
)
rlGrid <- expand.grid(cost = seq(0.001, 1, length.out = 20),
loss = "L2_Prmal",
epsilon = 0.01)
my_model <- train(Label ~ ., data = df_final,
method = 'regLogistic',
trControl = cctrl1,
preProc = c("center", "scale"),
tuneGrid = rlGrid)
my_model
warnings()
library(caret)
library(tidyverse)
df2 <- read.csv("C:/Users/qwert/Documents/microchip.csv", header=F)
head(df2)
# Save 20 rows for testing
reserved = df2 %>% sample_n(20)
df2_train = df2 %>% setdiff(reserved)
dim(reserved)
dim(df2_train)
# Save the test data. We will upload this  to our siny app
# to get predictions
write.csv(reserved, "C:/Users/qwert/Documents/test_data.csv", row.names = F)
# Name columns
names(df2_train) = c("Test1", "Test2", "Label")
# Visualize the data
cols <- c("0" = "red", "1" = "blue")
df2_train %>% ggplot(aes(x = Test1, y = Test2, color = factor(Label))) +
geom_point(size = 4, shape = 19, alpha = 0.6) +
scale_colour_manual(values = cols,labels = c("Failed", "Passed"), name = "Test Result")
# Feature Engineering
feature_mapping = function(df){
new_data = c()
for( i in 1:6){
for(j in 0:i){
temp = (df$Test1)^i+(df$Test2)^(i-j)
new_data = cbind(new_data, temp)
}
}
colnames(new_data) = paste0("V", 1:ncol(new_data))
new_data
}
mapped = feature_mapping(df2_train)
head(mapped)
df_final = cbind(df2_train, mapped)
dim(df_final)
df_final$Label = as.factor(df_final$Label)
levels(df_final$Label) <- c("Failed", "Passed")
# Building a reguralized logistic regression model using cross-validation and grid search
set.seed(0)
cctrl1 <- trainControl(method = "cv", number = 10,
allowParallel = T,
summaryFunction = twoClassSummary,
classProbs = T
)
rlGrid <- expand.grid(cost = seq(0.001, 1, length.out = 20),
loss = "L2_Primal",
epsilon = 0.01)
my_model <- train(Label ~ ., data = df_final,
method = 'regLogistic',
trControl = cctrl1,
preProc = c("center", "scale"),
tuneGrid = rlGrid)
install.packages("corrplot")
install.packages("leaflet")
install.packages("VIF")
install.packages("VIF")
shiny::runApp('RWorkdir/Automated Statistical Model/regression')
runApp('RWorkdir/Automated Statistical Model/regression')
library(readxl)
project1 <- read_xlsx("Project.xlsx",sheet = "Demographic Data", col_names=T)
head(project1)
project2 <- read_xlsx("Project.xlsx",sheet = "levelOfRecreationalFacilities", col_names=T)
shiny::runApp('RWorkdir/Automated Statistical Model/regression')
install.packages("dplyr")
install.packages("pillar")
shiny::runApp('RWorkdir/Automated Statistical Model/regression')
update.packages(ask=FALSE, checkBuilt = TRUE)
update.packages(ask=FALSE, checkBuilt = TRUE)
update.packages(ask=FALSE, checkBuilt = TRUE)
update.packages(ask=FALSE, checkBuilt = TRUE)
detach("package:shiny", unload=TRUE)
detach("package:shinydashboard", unload=TRUE)
detach("package:tidyverse", unload=TRUE)
detach("package:tidyr", unload=TRUE)
detach("package:tibble", unload=TRUE)
detach("package:stringr", unload=TRUE)
detach("package:stargazer", unload=TRUE)
detach("package:readr", unload=TRUE)
detach("package:purrr", unload=TRUE)
library("purrr", lib.loc="~/R/win-library/3.6")
detach("package:MASS", unload=TRUE)
detach("package:maps", unload=TRUE)
detach("package:leaflet", unload=TRUE)
detach("package:lattice", unload=TRUE)
detach("package:caret", unload=TRUE)
detach("package:corrplot", unload=TRUE)
detach("package:dplyr", unload=TRUE)
detach("package:DT", unload=TRUE)
detach("package:forcats", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
update.packages(ask=FALSE, checkBuilt = TRUE)
shiny::runApp('RWorkdir/Automated Statistical Model/regression')
update.packages(ask=FALSE, checkBuilt = TRUE)
update.packages(ask=FALSE, checkBuilt = TRUE)
update.packages(ask=FALSE, checkBuilt = TRUE)
shiny::runApp('RWorkdir/Automated Statistical Modelling and Data Mining/regression')
runApp('RWorkdir/Automated Statistical Modelling and Data Mining/regression')
shiny::runApp('RWorkdir/Shinydir2/STDNormRandom_ggplot2')
runApp('RWorkdir/Shinydir2/STDNormRandom_ggplot2')
runApp('RWorkdir/Shinydir2/STDNormRandom_ggplot2')
runApp('RWorkdir/Shinydir2/STDNormRandom_ggplot2')
runApp('RWorkdir/Shinydir2/STDNormRandom_ggplot2')
runApp('RWorkdir/Shinydir/UniformRandom')
runApp('RWorkdir/Shinydir/UniformRandom')
shiny::runApp('RWorkdir/Shinydir/UniformRandom')
ggplot(NULL,aes(x=x.values,y=dnorm(x.values,m=100,s=15))) +
geom_line() +
labs(x="IQ",y="f(IQ)") +
scale_x_continuous(breaks=sd.values,labels = sd.values) +
geom_segment((aes(x=sd.values,y=zeros9,xend =
sd.values,yend=dnorm(sd.values,m=100,s=15))),
linetype = "dashed") +
scale_y_continuous(expand = c(0,0))
# cumulative density function
pnorm(100,m=100,s=15) # returns the probality of a score less than x
pnorm(85,m=100,s=15)
pnorm(85,m=100,s=15, lower.tail = FALSE) # returns probaility of a score greater than x
# Plotting the cdf
ggplot(NULL,aes(x=x.values,y=pnorm(x.values,m=100,s=15))) +
geom_line() +
labs(x="IQ",y="f(IQ)") +
scale_x_continuous(breaks=sd.values,labels = sd.values) +
geom_segment((aes(x=sd.values,y=zeros9,xend =
sd.values,yend=pnorm(sd.values,m=100,s=15))),
linetype = "dashed") +
scale_y_continuous(expand = c(0,0))
# plotting cdf with quartiles
# replace the standard deviation values with quartiles
q.values <-round(qnorm(c(.25,.50,.75),m=100,s=15))
zeros3 <- c(0,0,0)
# Random sampling
rnorm(5,m=100,s=15)# generates 5 random numbers
round(qnorm(c(.25,.50,.75),m=100,s=15)) # best if rounded off
library("tigerstats")
rnorm(5,m=100,s=15)
pnormGC(c(85,100),region="between",m=100,s=15,graph=TRUE)
set.seed(7637060) # for reproducing randomization results
# Quantiles of normal distributions
# find a score that cuts off the area(to the left)
qnorm(0.1586553,m=100,s=15) # Inverse of the pnorm() function
rnorm(5,m=100,s=15)
qnorm(0.1586553,m=100,s=15, lower.tail = FALSE) # a score that cuts off area to the right
qnormGC(.1586553, region = "below",m=100,s=15, graph=TRUE) # tigerstats package
qnorm(c(0,.25,.50,.75,1.00),m=100,s=15) # quartiles # cdf never touches x-axis nor reache exact maximum
# finding the probability of a score between a lowwer bound and upper boound
any(grepl("tigerstats",installed.packages()))
ggplot(NULL,aes(x=x.values,y=pnorm(x.values,m=100,s=15))) +
geom_line() +
labs(x="IQ",y="f(IQ)") +
scale_x_continuous(breaks=q.values,labels = q.values) +
geom_segment((aes(x=q.values,y=zeros3,xend =
q.values,yend=pnorm(sd.values,m=100,s=15))),
linetype = "dashed") +
scale_y_continuous(expand = c(0,0))
set.seed(7637060)
rnorm(5,m=100,s=15)
# Standard normal distribution
# Don't specify the mean and te standard deviation
dnorm(0)
pnorm(0)
qnorm(c(.25,.50,.75))
# Standard normal distribution
# Don't specify the mean and te standard deviation
dnorm(0)
pnorm(0)
qnorm(c(.25,.50,.75))
update.packages(ask=FALSE, checkBuilt = TRUE)
shiny::runApp('RWorkdir/Automated Statistical Model/regression')
install.packages("broom")
install.packages("broom")
devtools::install_github("tidyverse/broom")
devtools::install_github("tidyverse/broom")
devtools::install_github("tidyverse/broom")
install.packages("broom" source=T)
install.packages("broom", source=T)
install.packages("broom")
shiny::runApp('RWorkdir/Automated Statistical Model/regression')
install.packages("broom")
install.packages("broom")
install.packages("broom", source = F)
install.packages("broom")
install.packages("broom")
install.packages("broom" version = 0.76)
install.packages("broom", version = 0.76)
install.packages("broom", INSTALL_opts = 0.76)
devtools::install_github("tidyverse/broom")
install.packages("broom")
install.packages("bench")
install.packages("C:/Users/qwert/Downloads/Compressed/broom_0.7.6.zip", repos = NULL, type = "win.binary")
runApp('RWorkdir/Automated Statistical Model/regression')
install.packages("dplyr")
shiny::runApp()
install.packages("C:/Users/qwert/Downloads/Compressed/dplyr_1.0.6.zip", repos = NULL, type = "win.binary")
